# ğŸ§  Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions

> _"Ever wished your photo could argue back in meetings? Weâ€™re not there yet... but this is dangerously close."_  
> â€” The Authors (probably)

---

## ğŸ“˜ About This Repository

Welcome to the **most expressive survey** on Talking Head Generation (THG) you'll ever read â€” or watch (spoiler: we include video results).

This repo accompanies the paper **â€œAdvancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functionsâ€**, authored by [Vineet Kumar Rakesh](mailto:vineet@vecc.gov.in), [Soumya Mazumdar](mailto:reachme@soumyamazumdar.com), [Research Pratim Maity](mailto:researchpratimmaity2004@gmail.com), [Sarbajit Pal](mailto:sarbajit@vecc.gov.in), [Amitabha Das](mailto:amitabhad.snsa@jadavpuruniversity.in), [Tapas Samanta](mailto:tsamanta@vecc.gov.in). 

This work was supported by the Variable Energy Cyclotron Centre (VECC), Department of Atomic Energy, Government of India and Homi Bhabha National Institute, Department of Atomic Energy, Government of India, which provided comprehensive facilities and technical support for the research and the Department of Atomic Energy, Government of India, for sponsoring the openâ€‘access publication of this work.

We explore 500+ research works from 2017 to 2025, dissect architectures like a neural coroner, evaluate datasets until they confess their secrets, and even benchmark some THG models for fun (and science).

---

## ğŸ§ª Whatâ€™s Inside

- ğŸ“š **Survey** of over 500 papers
- ğŸ—‚ï¸ **Categorization**: Audio, Video, Image, Text, 2D, 3D, GAN, NeRF, Transformers, etc.
- ğŸ“¦ **Datasets**: VoxCeleb, GRID, LRS3, CelebV, and more
- ğŸ§® **Evaluation Metrics**: SSIM, PSNR, LMD, CPBD, WER, LPIPS, CSIM
- ğŸ¯ **Loss Functions**: From pixel-wise pain to perceptual power
- ğŸ¤– **Code**: Evaluation & visual quality assessment scripts
- ğŸ¬ **Demo Videos**: See Wav2Lip and SadTalker in action (scroll ğŸ‘‡)
- âš ï¸ **Warnings**: DeepFakes are not your friends

---

## ğŸï¸ Sample Outputs

| Model | Output |
|-------|--------|
| **Wav2Lip** | ğŸ¥ [Watch](./GT_Wav2Lip.mp4) |
| **Output (Wav2Lip)** | ğŸ¥ [Watch](./Wav2Lip.mp4) |
| **SadTalker** | ğŸ¥ [Watch](./GT_SadTalker.mp4) |
| **Output (SadTalker)** | ğŸ¥ [Watch](./SadTalker.mp4) |

---

## âš™ï¸ Code

Coming soon in the `eval-scripts/` directory:

- `compute_metrics.py`: SSIM, PSNR, LMD, CPBD & many more evaluations
- `align_faces.py`: Align input/output for fair comparison
- `visualize_lipsync.py`: Compare generated vs ground-truth sync

Stay tuned, or just `git pull --force` every hour (not recommended).

---

## ğŸ“ˆ Benchmarks Snapshot (Sample)

| Model         | Dataset     | SSIM â†‘ | PSNR â†‘ | LMD â†“ |
|---------------|-------------|--------|--------|--------|
| Wav2Lip       | VoxCeleb2   | 0.74   | 32.5   | 1.21   |
| FOMM          | VoxCeleb1   | 0.68   | 29.8   | 1.49   |
| Face-vid2vid  | GRID        | 0.72   | 31.2   | 1.33   |

*â†‘: Higher is better | â†“: Lower is better â€” just like your blood pressure after debugging Wav2Lip*

---

## ğŸ§  Core Contributions (TL;DR for humans with deadlines)

1. **Unified taxonomy** for all talking-head paradigms.
2. **Cross-paradigm benchmarks** using public models.
3. **Dataset decoding**: what's worth your GPU time.
4. **Metric mayhem**: which one really captures realism?
5. **Loss function symphony**: the good, the bad, the not-so-differentiable.

---

## ğŸ”– Citation

```bibtex
@misc{rakesh2025talkingheadreview,
  title={Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions},
  author={Vineet Kumar Rakesh and Soumya Mazumdar and Research Pratim Maity and Sarbajit Pal and Amitabha Das and Tapas Samanta},
  year={2025},
  note={Bolbona}
}
